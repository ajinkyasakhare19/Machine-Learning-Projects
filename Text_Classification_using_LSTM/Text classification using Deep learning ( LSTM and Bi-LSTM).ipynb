{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis using Deep Learning - LSTM/BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qndnYAQpL6I8"
   },
   "source": [
    "In this task we will develop a system to detect irony in text. We will use the data from the SemEval-2018 task on irony detection. You should use the file `SemEval2018-T3-train-taskA.txt` from Blackboard it consists of examples as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEvckziyL6I9"
   },
   "source": [
    "```csv\n",
    "Tweet index     Label   Tweet text\n",
    "1       1       Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\n",
    "2       1       @mrdahl87 We are rumored to have talked to Erv's agent... and the Angels asked about Ed Escobar... that's hardly nothing    ;)\n",
    "3       1       Hey there! Nice to see you Minnesota/ND Winter Weather \n",
    "4       0       3 episodes left I'm dying over here\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "2P9K0hBP3cQm",
    "outputId": "5c4f3624-db27-4d2c-c656-9bd2cf9fdb69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4c747490-68ac-4309-a68d-5f2645fc11cc\" name=\"files[]\" multiple disabled />\n",
       "     <output id=\"result-4c747490-68ac-4309-a68d-5f2645fc11cc\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving SemEval2018-T3-train-taskA.txt to SemEval2018-T3-train-taskA (1).txt\n",
      "User uploaded file \"\" with length 9000 bytes\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "This section has been written to read the tweeter file into colab use after wards\n",
    "\n",
    "'''\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"\" with length 9000 bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwbHvVn5AJU_"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In This section wer are reading the text file containing the tweet details\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "tweets = pd.read_csv('SemEval2018-T3-train-taskA.txt',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "6FuyWbfuAdwB",
    "outputId": "cc5028ef-c418-4d77-e7e3-b0ab7739d175"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet index</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tweet text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sweet United Nations video. Just in time for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>@mrdahl87 We are rumored to have talked to Erv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Hey there! Nice to see you Minnesota/ND Winter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3 episodes left I'm dying over here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>I can't breathe! was chosen as the most notabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tweet index  Label                                         Tweet text\n",
       "0            1      1  Sweet United Nations video. Just in time for C...\n",
       "1            2      1  @mrdahl87 We are rumored to have talked to Erv...\n",
       "2            3      1  Hey there! Nice to see you Minnesota/ND Winter...\n",
       "3            4      0                3 episodes left I'm dying over here\n",
       "4            5      1  I can't breathe! was chosen as the most notabl..."
      ]
     },
     "execution_count": 428,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "Rlqej-fDUJmj",
    "outputId": "f4b4c298-42da-4cb6-ea58-362a666a01c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 429,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Importing NLTK library to use necessary packages\n",
    "\n",
    "'''\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "dvbODoKQ2kCe",
    "outputId": "d0c808e2-c967-415e-94d8-d729051e6f72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the dataset : 13460\n",
      "Number of piositive tweets : 1901\n",
      "Number of piositive tweets : 1916\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "As part of the question here we are reading the dataset and calculating the size of vocabulary and number of positive \n",
    "and negative examples\n",
    "\n",
    "count_words() : This funtion takes the dataframe as as input, make the words in lower case and calculates the total\n",
    "number of words in the whole data set.\n",
    "\n",
    "count_labels() : This function takes the dataframe as input and calculates the number of positive and negative tweets \n",
    "from the Label column\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def count_words(dframe):\n",
    "    texts = dframe['Tweet text'].str.lower()\n",
    "    all_txt = ' '.join(texts)\n",
    "    return len(set(nltk.word_tokenize(all_txt)))\n",
    "\n",
    "def count_labels(dframe):\n",
    "    positive_count=0\n",
    "    negative_count=0\n",
    "    for item in dframe['Label']:\n",
    "        if item == 1:\n",
    "            positive_count +=1\n",
    "        else:\n",
    "            negative_count +=1\n",
    "    return (positive_count,negative_count)\n",
    "\n",
    "size_of_dataset = count_words(tweets)\n",
    "pos_w_count,neg_count = count_labels(tweets)\n",
    "\n",
    "print('size of the dataset :',size_of_dataset)\n",
    "print('Number of piositive tweets :',pos_w_count)\n",
    "print('Number of piositive tweets :',neg_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4MtYIp6L63Zv",
    "outputId": "fd31d6f1-949b-4a2c-8af9-a0cf30a13e91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11697"
      ]
     },
     "execution_count": 446,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In This part we are preprocessing the tweets in order to remove unnecessary item which will not \n",
    "contribute much to our model.\n",
    "\n",
    "'''\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def text_preprocess_1(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('@[^\\s]+','',text)\n",
    "    text = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\W',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = \" \".join([i for i in text.split() if i not in STOPWORDS ])\n",
    "    return text\n",
    "tweets['Tweet text'] = tweets['Tweet text'].apply(lambda x: text_preprocess_1(x))\n",
    "size_of_dataset = count_words(tweets)\n",
    "size_of_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Jw15AHSM6knP",
    "outputId": "3d086d72-0e36-4ee0-d45c-94709bd3bf53"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 10035)"
      ]
     },
     "execution_count": 447,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In this part we have divided our data set in to two parts using Scikit Learn library train_test_split. He re we have seggregated\n",
    "the 80% of our data into traning set as we need more data to train our model and rest 20% to test our model.\n",
    "\n",
    "So in train_test_split we have passed the parameter test_size=0.20 which will do the job.\n",
    "Along with that we have converted our word data into numerical data by using CountVectorizer. This will basically\n",
    "converts the string into bag of words according the frequency of the word.\n",
    "\n",
    "For this Assignment we have also checd with Tf-Idf vecotorizer but CountVectorizer is performing a little better for our model.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "y = tweets['Label'].values\n",
    "X_train,X_test,y_train,y_test = train_test_split(tweets['Tweet text'],y,test_size=0.20,random_state=42)\n",
    "vectorizer = CountVectorizer()\n",
    "vecorized_x_train = vectorizer.fit_transform(X_train).toarray()\n",
    "vecorized_x_test = vectorizer.transform(X_test).toarray()\n",
    "vecorized_x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6kmP06Y-6bFx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "As asked in the question here we have implemented a function which will take predicted label and Actual label as input\n",
    "and give us the Accuracy,Precision,Recall and F1 score for us.\n",
    "\n",
    "score() : This is the function that calculates Accuracy,Precision,Recall and F1 score\n",
    "\n",
    "predict() : This function basically calculates the probability for our log-linear model and according to the \n",
    "probability it gives us the prediction.\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "def predict(X, y,weights_vector):\n",
    "    pred_prob = sigmoid(dot_pro(X,weights_vector))\n",
    "    y_pred = np.where(pred_prob>.5,1,0)\n",
    "    return y_pred\n",
    "def score(y_pred,y):\n",
    "    score = sum(y_pred == y) / len(y)\n",
    "    prcision = precision_score(y_pred,y)\n",
    "    recall = recall_score(y_pred,y)\n",
    "    fscore = f1_score(y_pred,y)\n",
    "    print('accuracy is:',score )\n",
    "    print('prcision is:',prcision )\n",
    "    print('recall is:',recall )\n",
    "    print('f1_score is:',fscore )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "sYYiwetp7F0T",
    "outputId": "2641c33b-77d2-4575-cd8e-0b5390cbf56f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: RuntimeWarning: divide by zero encountered in log\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 has finished. Cost is 2.71961310375693\n",
      "Epoch 100 has finished. Cost is 0.7753001096970733\n",
      "Epoch 200 has finished. Cost is 0.5468308949584724\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here we have implemented our Log-Linear model to classify the tweets being ironic or not.\n",
    "We have have taken the Sigmoid function to calculate the probability and used Cross-Entropy as our loss function.\n",
    "\n",
    "'''\n",
    "import numpy as np\n",
    "epochs = 200\n",
    "learning_rate = 0.001\n",
    "weights_vector = np.random.random(vecorized_x_train.shape[1])\n",
    "def dot_pro(x,weights):\n",
    "    return np.dot(x,weights)\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(-x))\n",
    "def cost_function(y_pred,Y):\n",
    "    return -Y*np.log(y_pred) - (1-Y)*np.log(1-y_pred)\n",
    "for ep in range(epochs+1):\n",
    "    cost = 0\n",
    "    for i in range(len(vecorized_x_train)):\n",
    "        X = vecorized_x_train[i]\n",
    "        y  = y_train[i]\n",
    "        y_pred = sigmoid(np.dot(X,weights_vector))\n",
    "        cost = cost_function(y_pred,y)\n",
    "        weights_vector = weights_vector - learning_rate*(y_pred - y)* X\n",
    "    if ep%100==0:\n",
    "        print (\"Epoch {} has finished. Cost is {}\".format(ep,cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "tm03D6BT7USN",
    "outputId": "0ae887d4-d765-41aa-f016-23c9507ae452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.5301047120418848\n",
      "prcision is: 0.5169082125603864\n",
      "recall is: 0.5737265415549598\n",
      "f1_score is: 0.5438373570520966\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After we trained our Log-Linear model using the weights we are predicting the values of our unseen\n",
    "Tweets, which we have kept aside for our testing.\n",
    "\n",
    "After we get the predicted labels for test data we are using score() function to get the Accuracy,precision,Recall\n",
    "and f1 score.\n",
    "\n",
    "'''\n",
    "\n",
    "prediction = predict(vecorized_x_test,y_test,weights_vector)\n",
    "score(prediction,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Deeplearning methods using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IvjAtxYuRSo9",
    "outputId": "f56d71e5-360c-4a3a-fa91-e3bc835ca1a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11699 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In this section we have implemented an Acceptor using Keras for classifying the tweets as Ironic or non ironic.\n",
    "We have followed the below steps t build our RNN.\n",
    "1. We have created a word dictionary of sequence using frequent 5000 words in the tweets ignoring some special chars.\n",
    "2.Then we have padded each sentences to a length of 33 which is giving us the best results. This we can change accoding to our \n",
    "model performance.\n",
    "3.Then we have transformed our Label to one hot vector\n",
    "4.Again we have done the split for training and Test data\n",
    "5.Lastly we have build our simple RNN model.\n",
    "'''\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NB_WORDS = 5000\n",
    "MAX_SEQUENCE_LENGTH = 33\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(tweets['Tweet text'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "LK5TauB6YpCJ",
    "outputId": "e1341ab6-e4d6-47b1-993b-ab623b1eb010"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (3817, 33)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 488,  829, 1611, ...,    0,    0,    0],\n",
       "       [3769, 3770, 1612, ...,    0,    0,    0],\n",
       "       [ 275,   71,   12, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [  35, 1997, 3596, ...,    0,    0,    0],\n",
       "       [1399, 3767, 1241, ...,    0,    0,    0],\n",
       "       [  63,  713, 1186, ...,    0,    0,    0]], dtype=int32)"
      ]
     },
     "execution_count": 459,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = tokenizer.texts_to_sequences(tweets['Tweet text'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH,padding='post')\n",
    "print('Shape of data tensor:', X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "wIttTjUQaEzx",
    "outputId": "976066bf-06f8-4ff1-b8dd-0065c84d0508"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       ...,\n",
       "       [1, 0],\n",
       "       [1, 0],\n",
       "       [1, 0]], dtype=uint8)"
      ]
     },
     "execution_count": 460,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Y = pd.get_dummies(tweets['Label']).values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "v99412_abuCT",
    "outputId": "b98a61fb-bc0b-416f-d413-4465f2b6e683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2671, 33) (2671, 2)\n",
      "(1146, 33) (1146, 2)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "saNXkrxObyz1",
    "outputId": "aed3cdd3-3330-439a-ba47-c0a438ad7eec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_86\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_85 (Embedding)     (None, 33, 100)           500000    \n",
      "_________________________________________________________________\n",
      "lstm_99 (LSTM)               (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 580,602\n",
      "Trainable params: 580,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#-----------RNN MODEL-----------#\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "rmhEhJh8b9aM",
    "outputId": "ee993ba0-092e-4984-c763-1921cbb914e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2403 samples, validate on 268 samples\n",
      "Epoch 1/20\n",
      "2403/2403 [==============================] - 26s 11ms/step - loss: 0.6933 - acc: 0.5169 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 2/20\n",
      "2403/2403 [==============================] - 4s 2ms/step - loss: 0.6932 - acc: 0.5073 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 3/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.6926 - acc: 0.5106 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 4/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.6926 - acc: 0.5056 - val_loss: 0.6922 - val_acc: 0.5224\n",
      "Epoch 5/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.6926 - acc: 0.5177 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 6/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.6498 - acc: 0.6184 - val_loss: 0.7326 - val_acc: 0.6231\n",
      "Epoch 7/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.5086 - acc: 0.7836 - val_loss: 0.7803 - val_acc: 0.5896\n",
      "Epoch 8/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.4891 - acc: 0.8044 - val_loss: 0.7257 - val_acc: 0.5858\n",
      "Epoch 9/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.4836 - acc: 0.8165 - val_loss: 0.8417 - val_acc: 0.5896\n",
      "Epoch 10/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.4576 - acc: 0.8248 - val_loss: 0.8103 - val_acc: 0.5896\n",
      "Epoch 11/20\n",
      "2403/2403 [==============================] - 4s 2ms/step - loss: 0.4446 - acc: 0.8365 - val_loss: 0.8143 - val_acc: 0.5970\n",
      "Epoch 12/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.4353 - acc: 0.8423 - val_loss: 0.9185 - val_acc: 0.6045\n",
      "Epoch 13/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.4235 - acc: 0.8481 - val_loss: 0.8770 - val_acc: 0.6045\n",
      "Epoch 14/20\n",
      "2403/2403 [==============================] - 4s 2ms/step - loss: 0.4170 - acc: 0.8535 - val_loss: 0.8878 - val_acc: 0.5970\n",
      "Epoch 15/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.4053 - acc: 0.8598 - val_loss: 0.8734 - val_acc: 0.5933\n",
      "Epoch 16/20\n",
      "2403/2403 [==============================] - 4s 2ms/step - loss: 0.4004 - acc: 0.8635 - val_loss: 0.8662 - val_acc: 0.5933\n",
      "Epoch 17/20\n",
      "2403/2403 [==============================] - 4s 2ms/step - loss: 0.4107 - acc: 0.8539 - val_loss: 0.7871 - val_acc: 0.5896\n",
      "Epoch 18/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.5665 - acc: 0.7291 - val_loss: 0.6858 - val_acc: 0.5634\n",
      "Epoch 19/20\n",
      "2403/2403 [==============================] - 5s 2ms/step - loss: 0.6141 - acc: 0.6583 - val_loss: 0.7070 - val_acc: 0.5896\n",
      "Epoch 20/20\n",
      "2403/2403 [==============================] - 4s 2ms/step - loss: 0.5991 - acc: 0.6775 - val_loss: 0.7065 - val_acc: 0.5896\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After building the RNN model we are fitting the model with the help of Training data.\n",
    "We are fitting the model in the batch of 64 with 20 epochs.\n",
    "'''\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "history = model.fit(X_train, Y_train, epochs=epochs,\n",
    "                    validation_split=0.1,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "4SLTYN_W6H0f",
    "outputId": "5c85c5dd-fc96-402e-d351-4ba750e04565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.512216404886562\n",
      "prcision is: 0.17017828200972449\n",
      "recall is: 0.6907894736842105\n",
      "f1_score is: 0.27308192457737324\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "After the model being trained, we pass our Test data to predict the Label for the same. Keras has Predict_classes method\n",
    "to calculate the same.\n",
    "\n",
    "After we get the prediction we are passing the same to our pre-built score() function to calculate the Accuracy,precision,Recall\n",
    "and F1 score.\n",
    "\n",
    "'''\n",
    "\n",
    "y_pred_test =  model.predict_classes(X_test, batch_size=batch_size, verbose=0)\n",
    "score(y_pred_test,np.argmax(Y_test,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yZucGZmL6JM"
   },
   "source": [
    "## Implementation of BiDirectional LSTM with Stacked LSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Improvements\n",
    "\n",
    "In this section we have tried to implement the concept of Birectional LSTM with stacked LSTM.\n",
    "\n",
    "#### Model Design:\n",
    "The new Model is designed with help of the concept(As being studied in the lectures) of Bidirectional LSTM and stacked LSTM.\n",
    "The basic idea of the BiRNN is to read the sequence of words both from the beggining and from the end,as both the sequence may be useful for any prediction.\n",
    "\n",
    "In BiRNN the model maintains two states,**S<sub>i</sub><sup>f</sup>(Forward State)** when we feed the sequence from the beginning  and **S<sub>i</sub><sup>b</sup>(Backward state)**, when we feed he sequence from the end. the output at a particular position is \n",
    "the concatenation of the two output vectors.\n",
    "\n",
    "Then  I have used the concept of Multilayer RNN(Stacked RNN) where we use the output of the one LSTM layer as the input of the next LSTM layer.These type of architecture is called Deep RNNs. \n",
    "Though there has been no solid theoritcal explanations for the better performance of the Deep RNNs, but practically it has been tried and tested which yielded better performances.\n",
    "\n",
    "In our case also when we used Deep RNNs, BiRNN+Multilayer LSTM it has given us better performance than our initial model.\n",
    "\n",
    "#### Process Run:\n",
    "As seen in the above our previous RNN model does not perform very well with some low Precision and F1. Then we used the same input to test our new model.\n",
    "\n",
    "We have used the same preprocessing steps as our previous models as to compare the results on a same ground. So we have taken most frquent 5000 words and converted each tweet into sequence.\n",
    "Then we have done the padding. After that we have split the tweets into Train(70%) and Test(30%) data using Scikit Learn.\n",
    "\n",
    "Then we have done Label into one hot ecoding to our Labels and used **categorical_crossentropy** as our loss function.Instead of dropout we have used Spatial Dropout which generally drops the entire entire 1D feature maps instead of individual elements.IN the end we have used Softmax activation function to get the probability of the particular input.\n",
    "\n",
    "Then we have fed the input to our Bidirectional LSTM. The output of the BiRNN then again fed to another LSTM layer. In our experiment we have found that our model has yielded better results than previous.\n",
    "\n",
    "#### Evaluation :\n",
    "\n",
    "After we built the model we passed our Testing data to for desired prediction.\n",
    "We have fit the model with Training data with batch size of and then passed Test data to get the  prediction values. Then the prediction values we fed the same to our pre built score() function to get all the required evaluation metrices.\n",
    "\n",
    "We have clearly see the our model gave us a 60% overall accuracy with 60% overall prcision,Recall and F1 score.\n",
    "\n",
    "We have completed our model for Tweet classification the manner. PFB the Model design and resul outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "qP6S0Dj2q7Va",
    "outputId": "d0fa7378-8a19-4f43-c304-d821d20c9380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_84 (Embedding)     (None, 33, 100)           500000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_37 (Spatia (None, 33, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 33, 256)           234496    \n",
      "_________________________________________________________________\n",
      "lstm_98 (LSTM)               (None, 128)               197120    \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 931,874\n",
      "Trainable params: 931,874\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Dropout,Embedding,LSTM,Flatten,GRU,SpatialDropout1D,Bidirectional\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model4.add(SpatialDropout1D(0.25))\n",
    "model4.add(Bidirectional(LSTM(128,return_sequences=True)))\n",
    "model4.add(LSTM(128))\n",
    "#model4.add(Dropout(0.2))\n",
    "model4.add(Dense(2, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mh3c-KxBEJCB"
   },
   "outputs": [],
   "source": [
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "colab_type": "code",
    "id": "8og5NyyHEf-2",
    "outputId": "00da5d88-1d63-49f6-be77-f8599595b3db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2403 samples, validate on 268 samples\n",
      "Epoch 1/20\n",
      "2403/2403 [==============================] - 30s 12ms/step - loss: 0.6938 - acc: 0.4973 - val_loss: 0.6929 - val_acc: 0.5224\n",
      "Epoch 2/20\n",
      "2403/2403 [==============================] - 7s 3ms/step - loss: 0.6927 - acc: 0.5185 - val_loss: 0.6921 - val_acc: 0.5224\n",
      "Epoch 3/20\n",
      "2403/2403 [==============================] - 7s 3ms/step - loss: 0.6929 - acc: 0.5189 - val_loss: 0.6926 - val_acc: 0.5224\n",
      "Epoch 4/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.6929 - acc: 0.5194 - val_loss: 0.6923 - val_acc: 0.5224\n",
      "Epoch 5/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.6307 - acc: 0.6434 - val_loss: 0.7260 - val_acc: 0.5821\n",
      "Epoch 6/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.3918 - acc: 0.8261 - val_loss: 0.7650 - val_acc: 0.6007\n",
      "Epoch 7/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.2727 - acc: 0.9039 - val_loss: 0.8963 - val_acc: 0.5597\n",
      "Epoch 8/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.1774 - acc: 0.9384 - val_loss: 1.2527 - val_acc: 0.5634\n",
      "Epoch 9/20\n",
      "2403/2403 [==============================] - 7s 3ms/step - loss: 0.1216 - acc: 0.9625 - val_loss: 1.2756 - val_acc: 0.5634\n",
      "Epoch 10/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0907 - acc: 0.9717 - val_loss: 1.6666 - val_acc: 0.5448\n",
      "Epoch 11/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0839 - acc: 0.9754 - val_loss: 1.9191 - val_acc: 0.5746\n",
      "Epoch 12/20\n",
      "2403/2403 [==============================] - 7s 3ms/step - loss: 0.0794 - acc: 0.9796 - val_loss: 1.5246 - val_acc: 0.5560\n",
      "Epoch 13/20\n",
      "2403/2403 [==============================] - 7s 3ms/step - loss: 0.0758 - acc: 0.9754 - val_loss: 1.8431 - val_acc: 0.5597\n",
      "Epoch 14/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0527 - acc: 0.9863 - val_loss: 2.1297 - val_acc: 0.5709\n",
      "Epoch 15/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0508 - acc: 0.9883 - val_loss: 1.9360 - val_acc: 0.5634\n",
      "Epoch 16/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0432 - acc: 0.9917 - val_loss: 2.0956 - val_acc: 0.5522\n",
      "Epoch 17/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0422 - acc: 0.9913 - val_loss: 2.1165 - val_acc: 0.5597\n",
      "Epoch 18/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0493 - acc: 0.9879 - val_loss: 1.7307 - val_acc: 0.5709\n",
      "Epoch 19/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0405 - acc: 0.9917 - val_loss: 2.2499 - val_acc: 0.5597\n",
      "Epoch 20/20\n",
      "2403/2403 [==============================] - 6s 3ms/step - loss: 0.0509 - acc: 0.9863 - val_loss: 1.9696 - val_acc: 0.5672\n"
     ]
    }
   ],
   "source": [
    "history4=model4.fit(X_train, Y_train, validation_split=0.1,epochs=20, batch_size=64, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "x59wkZjBEiwu",
    "outputId": "04849f13-7fc9-4695-a601-5ac7ef48dfe9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is: 0.6047120418848168\n",
      "prcision is: 0.5980551053484603\n",
      "recall is: 0.6428571428571429\n",
      "f1_score is: 0.619647355163728\n"
     ]
    }
   ],
   "source": [
    "y_pred_test =  model4.predict_classes(X_test, batch_size=batch_size, verbose=0)\n",
    "score(y_pred_test,np.argmax(Y_test,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gkAHzbdRVIVd"
   },
   "source": [
    "#### References:\n",
    "\n",
    "To complete the above task we have taken ideas from following references\n",
    "\n",
    "[1] Neural Network Methods for Natural Language Processing by Yoav Goldberg <Chapter 14>\n",
    "\n",
    "[2] https://www.kaggle.com/nafisur/keras-models-lstm-cnn-gru-bidirectional-glove\n",
    "\n",
    "[3] https://keras.rstudio.com/reference/layer_spatial_dropout_1d.html\n",
    "\n",
    "[4] https://github.com/susanli2016/NLP-with-Python/blob/master/Multi-Class%20Text%20Classification%20LSTM%20Consumer%20complaints.ipynb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Advanced_Topics_in_Natural_Language_Processing_Assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
